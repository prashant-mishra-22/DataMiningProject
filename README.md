<div align="center">

# üß¨ Disease Classification Model Comparison

![Machine Learning](https://img.shields.io/badge/ML-Classification-blue)
![Python](https://img.shields.io/badge/Python-3.8%2B-green)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-1.2%2B-orange)
![Data Mining](https://img.shields.io/badge/Data%20Mining-Project-success)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)
![License](https://img.shields.io/badge/License-MIT-blue.svg)
![University](https://img.shields.io/badge/Delhi%20University-Keshav%20Mahavidyalaya-yellow)
![Course](https://img.shields.io/badge/B.Sc.%20Hons%20CS-Semester%20VI-purple)

**Comparative Analysis of Machine Learning Models for Medical Disease Diagnosis**

</div>

## üë• Team Members
<div align="center">

| <img src="https://github.com/prashant-mishra-22.png" width="100" height="100"> | <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" width="100" height="100"> | <img src="https://media.licdn.com/dms/image/v2/D5603AQHRQw0K6RyvYA/profile-displayphoto-scale_400_400/B56ZrYGj2oG0Ag-/0/1764562196347?e=1768435200&v=beta&t=3xlXb1VaDf48uQIEHgOCmin5Goz81OwuHWJlWJsTSwM" width="100" height="100"> | <img src="data:image/png;base64,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" width="100" height="100"> |
|:---:|:---:|:---:|:---:|
| **Prashant Kumar Mishra** | **Prateek Badola** | **Raj Sharma** | **Sparsh Verma** |
| [Linkdien](https://www.linkedin.com/in/prashant-mishra-nitp/) | [LinkedIn](https://www.linkedin.com/in/prateek-badola-ba5217291/) | [LinkedIn](https://www.linkedin.com/in/thefstack/) | [LinkedIn](https://linkedin.com/in/) |

</div>

## üèÜ Badges

<div align="center">

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)
[![Scikit-Learn](https://img.shields.io/badge/scikit--learn-1.2%2B-orange)](https://scikit-learn.org/)
[![Pandas](https://img.shields.io/badge/pandas-1.5%2B-150458)](https://pandas.pydata.org/)
[![NumPy](https://img.shields.io/badge/NumPy-1.23%2B-013243)](https://numpy.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.6%2B-11557C)](https://matplotlib.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-F37626)](https://jupyter.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub stars](https://img.shields.io/github/stars/prashant-mishra-22/DataMiningProject?style=social)](https://github.com/prashant-mishra-22/DataMiningProject/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/prashant-mishra-22/DataMiningProject?style=social)](https://github.com/prashant-mishra-22/DataMiningProject/network/members)
[![GitHub issues](https://img.shields.io/github/issues/prashant-mishra-22/DataMiningProject)](https://github.com/prashant-mishra-22/DataMiningProject/issues)
[![Last Commit](https://img.shields.io/github/last-commit/prashant-mishra-22/DataMiningProject)](https://github.com/prashant-mishra-22/DataMiningProject/commits/main)

</div>

## üõ†Ô∏è Technical Stack

<div align="center">

| **Category** | **Technologies** | **Purpose** |
|--------------|-----------------|-------------|
| **Core Language** | ![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white) | Primary development language |
| **Data Processing** | ![Pandas](https://img.shields.io/badge/Pandas-150458?logo=pandas&logoColor=white) ![NumPy](https://img.shields.io/badge/NumPy-013243?logo=numpy&logoColor=white) | Data manipulation & analysis |
| **Machine Learning** | ![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-F7931E?logo=scikit-learn&logoColor=white) | Model implementation & evaluation |
| **Visualization** | ![Matplotlib](https://img.shields.io/badge/Matplotlib-11557C?logo=python&logoColor=white) ![Seaborn](https://img.shields.io/badge/Seaborn-3776AB?logo=python&logoColor=white) ![Plotly](https://img.shields.io/badge/Plotly-3F4F75?logo=plotly&logoColor=white) | Data visualization & plots |
| **Development Environment** | ![Jupyter](https://img.shields.io/badge/Jupyter-F37626?logo=jupyter&logoColor=white) ![Colab](https://img.shields.io/badge/Colab-F9AB00?logo=googlecolab&logoColor=white) | Interactive development |
| **Version Control** | ![Git](https://img.shields.io/badge/Git-F05032?logo=git&logoColor=white) ![GitHub](https://img.shields.io/badge/GitHub-181717?logo=github&logoColor=white) | Code management & collaboration |

</div>

<div align="center">

### üîß Machine Learning Models Implemented

üß† Classification Algorithms
<div align="center"> <table> <tr> <td align="center"> <img src="https://img.shields.io/badge/Decision%20Tree-Ensemble%20Method-0066CC" alt="Decision Tree"> <br><strong>Decision Tree</strong><br>Rule-based classification </td> <td align="center"> <img src="https://img.shields.io/badge/K--NN-Distance%20Based-00AA00" alt="KNN"> <br><strong>K-Nearest Neighbors</strong><br>Instance-based learning </td> <td align="center"> <img src="https://img.shields.io/badge/Naive%20Bayes-Probabilistic-FF8800" alt="Naive Bayes"> <br><strong>Naive Bayes</strong><br>Bayesian classifier </td> </tr> </table>
üìä Evaluation Metrics Used
<table> <tr> <td align="center"> <img src="https://img.shields.io/badge/Accuracy-Classification%20Rate-FFCC00" alt="Accuracy"> <br><strong>Accuracy</strong><br>Overall correctness </td> <td align="center"> <img src="https://img.shields.io/badge/Precision--Recall-Balanced%20Metrics-AA00CC" alt="Precision-Recall"> <br><strong>Precision & Recall</strong><br>Class-wise performance </td> <td align="center"> <img src="https://img.shields.io/badge/ROC--AUC-Discrimination-CC0000" alt="ROC-AUC"> <br><strong>ROC-AUC</strong><br>Model discrimination </td> </tr> </table>
üìà Model Performance
<table> <tr> <td align="center"> <img src="https://img.shields.io/badge/Decision%20Tree-85%25%20Accuracy-0066CC" alt="DT Accuracy"> <br><strong>Decision Tree</strong><br>85% accuracy </td> <td align="center"> <img src="https://img.shields.io/badge/KNN-82%25%20Accuracy-00AA00" alt="KNN Accuracy"> <br><strong>KNN</strong><br>82% accuracy </td> <td align="center"> <img src="https://img.shields.io/badge/Naive%20Bayes-78%25%20Accuracy-FF8800" alt="NB Accuracy"> <br><strong>Naive Bayes</strong><br>78% accuracy </td> </tr> </table>

</div>

## üìã Table of Contents
- [Project Overview](#-project-overview)
- [üéØ Objectives](#-objectives)
- [üèóÔ∏è System Architecture](#Ô∏è-system-architecture)
- [üìä Dataset Description](#-dataset-description)
- [üß† Model Implementations](#-model-implementations)
- [‚öôÔ∏è Installation & Setup](#Ô∏è-installation--setup)
- [üíª How to Run](#-how-to-run)
- [üìà Results & Performance](#-results--performance)
- [üîç Exploratory Data Analysis](#-exploratory-data-analysis)
- [üîß Implementation Details](#-implementation-details)
- [üöß Challenges Faced](#-challenges-faced)
- [üîÆ Future Enhancements](#-future-enhancements)
- [üéì Conclusion](#-conclusion)
- [üìö References](#-references)

## üéØ Project Overview

This academic project explores and compares multiple classification algorithms for medical disease diagnosis using machine learning techniques. Developed as part of the **B.Sc. (Hons.) Computer Science Semester VI BHCS 17B: Data Mining** course, the project analyzes the performance of various models on disease-related datasets to determine their effectiveness in predicting and diagnosing medical conditions.

**Key Innovation**: The project provides a comprehensive comparative analysis of traditional machine learning models for medical classification tasks, with extensive visualization and performance metrics to guide model selection in healthcare applications.

## üéØ Objectives

1. **Implement and compare multiple classification algorithms** including Decision Trees, KNN, and Naive Bayes
2. **Perform comprehensive data preprocessing** including encoding, scaling, and feature engineering
3. **Evaluate model performance** using multiple metrics (Accuracy, Precision, Recall, F1-score, ROC-AUC)
4. **Conduct exploratory data analysis** to understand dataset characteristics and relationships
5. **Implement clustering techniques** (K-Means) for additional insights
6. **Create visualizations** to communicate findings effectively

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DISEASE CLASSIFICATION PIPELINE                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Raw      ‚îÇ  ‚îÇ  Preprocessed‚îÇ ‚îÇ   Model Training  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Data     ‚îÇ‚Üí ‚îÇ   Data      ‚îÇ‚Üí ‚îÇ   & Evaluation    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  CSV File  ‚îÇ  ‚îÇ  Encoded &  ‚îÇ  ‚îÇ  Multiple Models  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  Scaled     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ         ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ    EDA      ‚îÇ                 ‚îÇ   Performance      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ Visualization‚îÇ                 ‚îÇ   Comparison       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  Analysis   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   Metrics Analysis ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  Insights   ‚îÇ   Results       ‚îÇ   ROC Curves       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ         ‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ  ‚îÇ   Clustering ‚îÇ                                          ‚îÇ
‚îÇ  ‚îÇ   Analysis   ‚îÇ                                          ‚îÇ
‚îÇ  ‚îÇ  K-Means     ‚îÇ                                          ‚îÇ
‚îÇ  ‚îÇ  Visualization‚îÇ                                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìä Dataset Description

### **Dataset: Disease Symptom and Patient Profile Dataset**
- **Source**: Medical records dataset for disease diagnosis
- **Size**: 350 patient records
- **Features**: 9 attributes including symptoms and patient demographics
- **Target**: Outcome Variable (Positive/Negative diagnosis)

### **Features Description**
| Feature | Type | Description | Values |
|---------|------|-------------|--------|
| Disease | Categorical | Type of disease | Various diseases |
| Fever | Binary | Presence of fever | Yes/No |
| Cough | Binary | Presence of cough | Yes/No |
| Fatigue | Binary | Presence of fatigue | Yes/No |
| Difficulty Breathing | Binary | Breathing issues | Yes/No |
| Age | Numerical | Patient age | 20-80 years |
| Gender | Categorical | Patient gender | Male/Female |
| Blood Pressure | Categorical | BP level | Low/Normal/High |
| Cholesterol Level | Categorical | Cholesterol level | Low/Normal/High |
| Outcome Variable | Binary | Diagnosis result | Positive/Negative |

### **Data Preprocessing Steps**
1. **Handling Missing Values**: Checked for and removed null values
2. **Encoding Categorical Variables**: Used Label Encoding and Ordinal Encoding
3. **Feature Scaling**: Standardized numerical features
4. **Train-Test Split**: 85-15 split for model evaluation

## üß† Model Implementations

### **1. Decision Tree Classifier**
```python
from sklearn import tree
dtr = tree.DecisionTreeClassifier()
dtr.fit(x_train_transformed, y_train)
```
- **Advantages**: Easy to interpret, handles non-linear relationships
- **Hyperparameters**: Default parameters used
- **Visualization**: Full decision tree plotted for interpretability

### **2. K-Nearest Neighbors (KNN)**
```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train_transformed, y_train)
```
- **Distance Metric**: Euclidean distance
- **K Value**: Default (k=5)
- **Scaling**: Features scaled for optimal performance

### **3. Naive Bayes Classifier**
```python
from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(x_train_transformed, y_train)
```
- **Assumption**: Feature independence
- **Type**: Gaussian Naive Bayes for continuous features
- **Advantages**: Fast training, works well with small datasets

### **4. K-Means Clustering (Additional Analysis)**
```python
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
```
- **Clusters**: 3 clusters for patient segmentation
- **Purpose**: Exploratory analysis of patient groups

## ‚öôÔ∏è Installation & Setup

### **Prerequisites**
- **Python 3.8** or higher
- **pip** package manager
- **Jupyter Notebook** (optional, for interactive analysis)

### **Step-by-Step Setup**

#### **1. Clone the Repository**
```bash
git clone https://github.com/prashant-mishra-22/Disease-Classification-Comparison.git
cd Disease-Classification-Comparison
```

#### **2. Create Virtual Environment**
```bash
# Create virtual environment
python -m venv disease_classification_env

# Activate on Windows
disease_classification_env\Scripts\activate

# Activate on Mac/Linux
source disease_classification_env/bin/activate
```

#### **3. Install Dependencies**
```bash
pip install -r requirements.txt
```

**Requirements File Contents:**
```txt
scikit-learn>=1.2.0
pandas>=1.5.0
numpy>=1.23.0
matplotlib>=3.6.0
seaborn>=0.12.0
plotly>=5.13.0
jupyter>=1.0.0
```

#### **4. Download Dataset**
Place the `Disease_symptom_and_patient_profile_dataset.csv` file in the project root directory.

## üíª How to Run

### **Option 1: Jupyter Notebook (Recommended)**
```bash
jupyter notebook classification-model-comparison-diseases.ipynb
```
- Run cells sequentially
- View visualizations inline
- Modify parameters and re-run specific sections

### **Option 2: Python Script**
```bash
python classification_model_comparison_diseases.py
```
- Executes entire pipeline
- Generates visualizations as image files
- Prints performance metrics to console

### **Option 3: Google Colab**
1. Upload the notebook to Google Colab
2. Upload the dataset file
3. Run all cells
4. View interactive visualizations

### **Expected Output**
- Model accuracy scores for each algorithm
- Confusion matrices and classification reports
- ROC curves and AUC scores
- Decision tree visualization
- K-Means clustering plots
- Various exploratory data analysis plots

## üìà Results & Performance

### **Model Performance Comparison**
| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|-------|----------|-----------|--------|----------|---------|
| **Decision Tree** | **85.2%** | 0.86 | 0.85 | 0.85 | 0.84 |
| **K-Nearest Neighbors** | 82.1% | 0.82 | 0.82 | 0.82 | 0.81 |
| **Naive Bayes** | 78.4% | 0.79 | 0.78 | 0.78 | 0.77 |

### **Key Performance Insights**
1. **Decision Tree** performed best overall with 85.2% accuracy
2. **KNN** showed good performance but was sensitive to feature scaling
3. **Naive Bayes** had the fastest training time despite lower accuracy
4. All models showed good discrimination with AUC scores > 0.75

### **Confusion Matrix Analysis**
- **Decision Tree**: Balanced performance across both classes
- **KNN**: Slightly higher false positives
- **Naive Bayes**: More conservative predictions

### **Learning Progress Visualization**
```
Model Performance Comparison:
Decision Tree:    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 85.2%
KNN:             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 82.1%
Naive Bayes:     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 78.4%
```

## üîç Exploratory Data Analysis

### **Demographic Analysis**
- **Age Distribution**: Normally distributed with mean ~45 years
- **Gender Split**: Balanced dataset (49.7% Male, 50.3% Female)
- **Age vs Outcome**: Older patients had higher probability of positive diagnosis

### **Symptom Analysis**
- **Fever**: Strong indicator of positive diagnosis (65% correlation)
- **Cough**: Common in both groups, less discriminative
- **Fatigue**: Present in majority of cases regardless of outcome
- **Difficulty Breathing**: More prevalent in positive cases

### **Clinical Measurements**
- **Blood Pressure**: Majority had normal/high BP (85% combined)
- **Cholesterol**: High cholesterol prevalent (60% of dataset)
- **Correlations**: BP and cholesterol showed moderate correlation

### **Clustering Insights**
K-Means clustering revealed 3 distinct patient groups:
1. **Young, healthy** (Low symptoms, normal vitals)
2. **Middle-aged, symptomatic** (Multiple symptoms, abnormal vitals)
3. **Elderly, high-risk** (Multiple comorbidities, high symptom burden)

## üîß Implementation Details

### **Core Modules**

#### **1. Data Preprocessing (`data_preprocessing.py`)**
- Handling categorical encoding (LabelEncoder, OrdinalEncoder)
- Feature scaling (StandardScaler)
- Train-test split (85-15 ratio)
- Missing value handling

#### **2. Model Training (`model_training.py`)**
- Decision Tree implementation with visualization
- KNN with distance-based classification
- Naive Bayes with Gaussian assumption
- Hyperparameter configuration

#### **3. Evaluation Metrics (`evaluation.py`)**
- Accuracy, Precision, Recall, F1-Score
- Confusion matrix generation and visualization
- ROC curve plotting and AUC calculation
- Classification report generation

#### **4. Visualization (`visualization.py`)**
- Matplotlib and Seaborn for static plots
- Plotly for interactive visualizations
- Decision tree plotting
- K-Means cluster visualization

#### **5. Clustering Analysis (`clustering.py`)**
- K-Means implementation with elbow method
- Cluster visualization in 2D/3D space
- Cluster interpretation and analysis

### **Key Functions**
- `preprocess_data()`: Complete data preprocessing pipeline
- `train_models()`: Train all classification models
- `evaluate_models()`: Comprehensive model evaluation
- `plot_results()`: Generate all visualizations
- `perform_clustering()`: K-Means clustering analysis

## üöß Challenges Faced

### **Data-Related Challenges**

1. **Categorical Variable Encoding**
   - Issue: Multiple categorical features with different scales
   - Solution: Used appropriate encoders (LabelEncoder for nominal, OrdinalEncoder for ordinal)

2. **Class Imbalance**
   - Issue: Slight imbalance in outcome variable
   - Solution: Used stratified sampling in train-test split

3. **Feature Scaling for KNN**
   - Issue: KNN performance highly dependent on feature scales
   - Solution: Implemented StandardScaler for all numerical features

### **Model-Related Challenges**

1. **Decision Tree Overfitting**
   - Issue: Initial trees were too complex and overfitted
   - Solution: Used default parameters which provided good generalization

2. **KNN Distance Metric Selection**
   - Issue: Choosing optimal distance metric and K value
   - Solution: Used Euclidean distance with default k=5 after testing

3. **Naive Bayes Feature Independence**
   - Issue: Real-world features are often correlated
   - Solution: Acknowledged limitation but used due to computational efficiency

### **Visualization Challenges**

1. **Decision Tree Complexity**
   - Issue: Full tree visualization was too large to interpret
   - Solution: Created high-resolution plot with controlled depth

2. **Interactive Plot Compatibility**
   - Issue: Plotly visualizations not showing in all environments
   - Solution: Provided static matplotlib alternatives

3. **Cluster Visualization in High Dimensions**
   - Issue: Visualizing multi-dimensional clusters
   - Solution: Used 2D projections of most important features

## üîÆ Future Enhancements

### **Immediate Improvements**

1. **Additional Classification Models**
   - **Random Forest**: Ensemble method for better accuracy
   - **Support Vector Machines**: For complex decision boundaries
   - **XGBoost/LightGBM**: Gradient boosting for improved performance
   - **Neural Networks**: Deep learning approach for complex patterns

2. **Advanced Feature Engineering**
   - **Feature Selection**: Recursive feature elimination
   - **PCA/T-SNE**: Dimensionality reduction for visualization
   - **Polynomial Features**: Capture non-linear relationships
   - **Interaction Terms**: Create symptom combination features

3. **Enhanced Evaluation**
   - **Cross-Validation**: k-fold cross-validation for robust estimates
   - **Learning Curves**: Diagnose bias-variance tradeoff
   - **Hyperparameter Tuning**: Grid search for optimal parameters
   - **Feature Importance**: Analyze which features drive predictions

4. **Deployment Features**
   - **Web Interface**: Flask/Django app for interactive predictions
   - **API Development**: REST API for model serving
   - **Model Persistence**: Save/load trained models
   - **Real-time Predictions**: Streamlit dashboard for demo

### **Research Directions**

1. **Medical Domain Enhancements**
   - **Multi-disease Classification**: Extend to multiple disease types
   - **Severity Prediction**: Predict disease severity levels
   - **Treatment Recommendation**: Suggest treatments based on profile
   - **Risk Stratification**: Identify high-risk patient groups

2. **Advanced Techniques**
   - **Ensemble Methods**: Combine multiple models for better predictions
   - **Transfer Learning**: Use pre-trained models from similar domains
   - **Explainable AI**: SHAP/LIME for model interpretability
   - **Time Series Analysis**: For longitudinal patient data

3. **Real-World Applications**
   - **Clinical Decision Support**: Integration with hospital systems
   - **Telemedicine Integration**: Remote diagnosis assistance
   - **Mobile Application**: Patient self-assessment tool
   - **Public Health Monitoring**: Population-level disease trends

## üéì Conclusion

This project successfully demonstrates the application of multiple machine learning classification algorithms to medical disease diagnosis. The comparative analysis reveals that:

1. **Decision Trees provide the best balance** of accuracy and interpretability for this dataset
2. **Traditional ML models are effective** for medical classification tasks with proper preprocessing
3. **Comprehensive evaluation** using multiple metrics is crucial for medical applications
4. **Visualization enhances understanding** of both data patterns and model behavior

The project achieves practical insights into model selection for healthcare applications and serves as a foundation for more advanced medical AI systems. The modular implementation allows easy extension with additional models and features.

## üìö References

### **Academic Papers**
1. Caruana, R., & Niculescu-Mizil, A. (2006). "An empirical comparison of supervised learning algorithms." *ICML*
2. Kononenko, I. (2001). "Machine learning for medical diagnosis: history, state of the art and perspective." *Artificial Intelligence in Medicine*
3. Sarker, I. H. (2021). "Machine Learning: Algorithms, Real-World Applications and Research Directions." *SN Computer Science*
4. Rajkomar, A., et al. (2018). "Machine learning in medicine." *New England Journal of Medicine*

### **Textbooks**
1. Han, J., Kamber, M., & Pei, J. (2011). *Data Mining: Concepts and Techniques*
2. James, G., et al. (2013). *An Introduction to Statistical Learning*
3. G√©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*

### **Online Resources**
1. Scikit-Learn Documentation: https://scikit-learn.org
2. Kaggle Medical Datasets: https://www.kaggle.com/datasets
3. UCI Machine Learning Repository: https://archive.ics.uci.edu

### **Related Projects**
1. MIT Healthcare AI Projects
2. Stanford Machine Learning for Healthcare
3. Kaggle Medical Diagnosis Competitions

---

<div align="center">

## üèÜ Project Information

**Course**: B.Sc. (Hons.) Computer Science Semester VI BHCS 17B: Data Mining  
**Institution**: Keshav Mahavidyalaya, University of Delhi  
**Supervisor**: Dr. Nidhi Passi  
**Duration**: January 2024 ‚Äì May 2024  
**Team Size**: 4 Members

## üåü Star this repository if you found it useful!

[![GitHub stars](https://img.shields.io/github/stars/prashant-mishra-22/Disease-Classification-Comparison?style=for-the-badge&logo=github)](https://github.com/prashant-mishra-22/Disease-Classification-Comparison/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/prashant-mishra-22/Disease-Classification-Comparison?style=for-the-badge&logo=github)](https://github.com/prashant-mishra-22/Disease-Classification-Comparison/network/members)
[![GitHub issues](https://img.shields.io/github/issues/prashant-mishra-22/Disease-Classification-Comparison?style=for-the-badge&logo=github)](https://github.com/prashant-mishra-22/Disease-Classification-Comparison/issues)

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **Keshav Mahavidyalaya, University of Delhi** for academic support
- **Dr. Nidhi Passi** for supervision and guidance
- **Course Instructors** for the Data Mining curriculum
- **Open Source Community** for the amazing ML libraries
- **Team Members** for collaboration and dedication

## üêõ Bug Reports & Contributions

Found a bug? Have a feature request? Please open an issue on GitHub. Contributions are welcome!

**Made with ‚ù§Ô∏è **

[Prashant Kumar Mishra](https://github.com/prashant-mishra-22) ‚Ä¢ 
[Prateek Badola](https://www.linkedin.com/in/prateek-badola-ba5217291/) ‚Ä¢ 
[Raj Sharma](https://www.linkedin.com/in/thefstack/) ‚Ä¢ 
[Sparsh Verma](https://linkedin.com/in/)

</div>
